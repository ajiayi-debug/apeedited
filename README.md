##Overview:
This project involved revamping the APE codebase to integrate the Alpaca-7B model, resulting in cost reductions and increased efficiency. The effort was inspired by the study “Large Language Models Are Human-Level Prompt Engineers.”

##Key Achievements:

 • Successfully replaced OpenAI’s GPT-3 with Alpaca-7B in a Linux environment, optimizing resource utilization.
 • Innovated data transformation by converting CSV datasets to LaTeX tables using few-shot prompting, surpassing traditional zero-shot methods in prompt generation efficiency.
 • Applied principles from “Improving Few-Shot Prompts with Relevant Static Analysis Products” to fine-tune the APE-generated prompts, enhancing the quality and effectiveness of the output.
 • Developed an optimization for APE that incorporates semantic facts into the data, significantly improving the accuracy of the generated prompts.
 • Conducted a comparative analysis that confirmed the inclusion of semantic facts leads to a substantial increase in the accuracy of data to LaTeX table conversion.

##Conclusion:
The project demonstrates the potential of few-shot prompting combined with semantic facts to automate and improve data generation processes.
